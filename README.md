# ğŸ§  ARXIV RAG QA â€” Retrieval-Augmented Generation System

> **A modular RAG pipeline** that fetches, parses, and indexes research papers from **arXiv**, then answers questions using hybrid retrieval (dense + BM25 + HyDE + reranking) and an integrated **LLM backend** (Groq or compatible).
>
> ğŸš€ Deployable via **FastAPI** or **Gradio UI**, and fully containerized for reproducible setups.

---

## ğŸ§© Architecture Overview

```text
arxiv â†’ PDF/Text extraction â†’ Chunking â†’ Embedding â†’ Vector store
â†’ Multi-retriever search â†’ Reranking â†’ LLM â†’ API/UI output
```

**Core Components:**

* ğŸ“„ `phase1`: Fetch papers from arXiv
* ğŸ§¹ `phase2`: Extract and clean text
* ğŸ§© `phase3`: Chunk + Embed + Index
* ğŸ” `phase4`: Query (CLI or API)
* âš™ï¸ `FastAPI` backend â€” REST endpoints
* ğŸ¨ `Gradio` interface â€” interactive Q&A

---

## ğŸ§° Tech Stack

| Category         | Tools                                        |
| ---------------- | -------------------------------------------- |
| **Embedding**    | Sentence-Transformers (`all-mpnet-base-v2`)  |
| **Vector Store** | ChromaDB (persistent client)                 |
| **Retriever**    | Dense, BM25, Hybrid, HyDE, Cross-Encoder     |
| **Backend**      | FastAPI, Uvicorn                             |
| **UI**           | Gradio                                       |
| **Infra**        | Docker, Python 3.11                          |
| **LLM**          | Groq API (or any OpenAI-compatible endpoint) |

---

## âš™ï¸ Setup â€” Local Development

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/AliceLaguri404/ARXIV_RAG_QA.git
cd ARXIV_RAG_QA
```

### 2ï¸âƒ£ Create and activate a virtual environment

```bash
python3 -m venv .venv
source .venv/bin/activate   # macOS / Linux
```

### 3ï¸âƒ£ Install dependencies

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

### 4ï¸âƒ£ Set up your `.env`

Create `.env` in the project root (or copy from `.env.sample`):

```bash
GROQ_API_KEY=sk-xxxxxxxxxxxx
GROQ_MODEL=llama-3.1-8b-instant
PUBLIC_URL_BASE=http://127.0.0.1:8000
```

---

## ğŸ§® Pipeline Phases (Local Run)

Run from project **root** (each as a module under `src/`):

```bash
# 1ï¸âƒ£ Fetch from arXiv
python -m src.scripts.phase1_run

# 2ï¸âƒ£ Extract and clean text
python -m src.scripts.phase2_run

# 3ï¸âƒ£ Chunk + Embed + Index
python -m src.scripts.phase3_run

# 4ï¸âƒ£ Query CLI
python -m src.scripts.phase4_run
```

---

## âš¡ API & UI (Local Run)

### â–¶ï¸ Run FastAPI

```bash
uvicorn src.app.main:app --reload --port 8000
```

Then open: [http://127.0.0.1:8000/docs]

### ğŸ¨ Run Gradio UI

```bash
python -m src.ui.gradio_app
```

Then open: [http://127.0.0.1:7860]

---

## ğŸ³ Docker Deployment

### ğŸ”§ 1. Build the image

```bash
docker build -t arxiv_rag_qa:latest .
```

### ğŸ“¦ 2. Run FastAPI (default)

```bash
mkdir -p ./data ./cache
docker run --rm -it \
  -p 8000:8000 \
  -v "$(pwd)/data:/data" \
  -v "$(pwd)/cache:/cache" \
  -e CHROMA_PERSIST_PATH=/data/vectorstore \
  arxiv_rag_qa:latest
```

â†’ Open: [http://127.0.0.1:8000]

### ğŸ–¥ 3. Run Gradio UI

```bash
docker run --rm -it \
  -p 7860:7860 \
  -v "$(pwd)/data:/data" \
  -v "$(pwd)/cache:/cache" \
  -e RUN_MODE=gradio \
  arxiv_rag_qa:latest
```

â†’ Open: [http://127.0.0.1:7860]

### ğŸ§  4. Run ingestion pipeline (inside container)

```bash
docker run --rm -it \
  -v "$(pwd)/data:/data" \
  -v "$(pwd)/cache:/cache" \
  arxiv_rag_qa:latest \
  python -m src.scripts.phase3_run
```

---

## ğŸ“ Project Structure

```
src/
 â”œâ”€â”€ app/                  # FastAPI backend
 â”œâ”€â”€ ui/                   # Gradio interface
 â”œâ”€â”€ scripts/              # Pipeline phases
 â”œâ”€â”€ chunker/              # Semantic/recursive chunking
 â”œâ”€â”€ embeddings/           # SentenceTransformer-based encoder
 â”œâ”€â”€ vector_store/         # ChromaDB client
 â””â”€â”€ qa/                   # LLM runner + retriever logic
data/
 â”œâ”€â”€ raw/                  # Downloaded PDFs
 â”œâ”€â”€ processed/            # Extracted and chunked text
 â””â”€â”€ vectorstore/          # Chroma persistent storage
docker-entrypoint.sh
Dockerfile
requirements.txt
.env
```

---

## ğŸ§© Environment Variables (Key)

| Variable                        | Description                | Example                |
| ------------------------------- | -------------------------- | ---------------------- |
| `GROQ_API_KEY`                  | API key for LLM backend    | `sk-xxxxxx`            |
| `GROQ_MODEL`                    | Model name                 | `llama-3.1-8b-instant` |
| `CHROMA_PERSIST_PATH`           | ChromaDB storage directory | `/data/vectorstore`    |
| `RUN_MODE`                      | `api` or `gradio`          | `gradio`               |
| `HF_HOME`, `TRANSFORMERS_CACHE` | Model cache path           | `/cache/huggingface`   |

---

## ğŸš€ Quick Commands

| Task           | Command                                                          |
| -------------- | ---------------------------------------------------------------- |
| Rebuild image  | `docker build --no-cache -t arxiv_rag_qa:latest .`               |
| Run API        | `docker run -p 8000:8000 arxiv_rag_qa:latest`                    |
| Run Gradio     | `docker run -p 7860:7860 -e RUN_MODE=gradio arxiv_rag_qa:latest` |
| Ingest papers  | `python -m src.scripts.phase3_run`                               |
| Run all phases | `make pipeline` *(optional if you add a Makefile)*               |

---

## ğŸ§‘â€ğŸ’» Future Enhancements

* [ ] Vector compression with FAISS + IVF
* [ ] Caching for re-embedding skip
* [ ] Multi-LLM evaluation metrics
* [ ] CI/CD pipeline with Docker Compose
* [ ] Auto-sync with arXiv RSS feeds

---
